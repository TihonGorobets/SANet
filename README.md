# SAN Schedule Automation

Automated system that fetches the **Zarządzanie** timetable PDF from the
[Społeczna Akademia Nauk – Warszawa](https://san.edu.pl/plany-zajec-warszawa/studia-stacjonarne)
schedule page every day at 06:00, detects changes and — when the PDF has
updated — rebuilds `zarzadzanie.html` so the website always shows the latest
times and rooms.

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│  Trigger: cron (06:00) or GitHub Actions (schedule / workflow_dispatch) │
└───────────────────────────────┬─────────────────────────────────────┘
                                │
                    ┌───────────▼──────────┐
                    │   scraper/main.py    │  ← orchestrator
                    └───────────┬──────────┘
          ┌────────────┬────────┴────────┬────────────┐
          ▼            ▼                 ▼            ▼
    fetcher.py   detector.py       parser.py    generator.py
      │               │                │              │
   Pages +       SHA-256 hash     pdfplumber     Jinja-like
   PDF DL        compare /        table +        HTML builder
                 persist          text parse          │
                                       │              │
                               ┌───────▼──────────────▼──────┐
                               │       database.py            │
                               │   SQLite  •  data/schedule.db│
                               └─────────────────────────────┘
```

### Flow

1. **Fetch** — `fetcher.find_pdf_url()` scrapes the schedule page, finds the
   PDF link whose text contains *"zarządzanie"* and returns its URL.
2. **Download** — `fetcher.download_pdf()` streams the PDF to `data/pdfs/`
   with automatic retries.
3. **Detect** — `detector.has_changed()` computes the SHA-256 hash of the
   downloaded PDF and compares it against `data/last_hash.txt`.
   - **No change** → exit 0, nothing else happens.
   - **Changed / first run** → continue.
4. **Parse** — `parser.parse_pdf()` uses `pdfplumber` to extract schedule
   tables for the three configured Zarządzanie groups.
5. **Store** — `database.clear_schedule()` removes stale rows, then
   `database.insert_entries()` bulk-inserts all fresh entries.
6. **Generate** — `generator.generate_html()` reads from SQLite and writes
   `zarzadzanie.html` using the same CSS/JS as `schedule.html`.

---

## Project Structure

```
SANet/
├── schedule.html         # ← auto-generated by this system (main page)
├── css/styles.css
├── js/app.js
├── js/whiteboard.js
│
├── scraper/
│   ├── __init__.py
│   ├── config.py            # All config constants / paths
│   ├── fetcher.py           # HTTP downloader
│   ├── detector.py          # SHA-256 change detection
│   ├── parser.py            # pdfplumber PDF parser
│   ├── database.py          # SQLite layer
│   ├── generator.py         # HTML generator
│   ├── logging_setup.py     # Rotating file + coloured console logger
│   └── main.py              # CLI entry point & orchestrator
│
├── data/
│   ├── schedule.db          # SQLite database (auto-created)
│   ├── last_hash.txt        # Stored hash (auto-created)
│   └── pdfs/                # Downloaded PDFs
│
├── logs/
│   └── schedule_update.log  # Rotating log file
│
├── .github/
│   └── workflows/
│       └── daily_update.yml # GitHub Actions schedule
│
├── install_cron.sh          # One-shot cron registrar
├── requirements.txt
└── README.md
```

---

## Setup

### 1 — Python environment

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2 — Configure target groups

Open `scraper/config.py` and set `TARGET_GROUPS` to the **exact** group
identifiers that appear in the PDF (e.g. `"ZAR_1"`, `"Zarządzanie I"`):

```python
TARGET_GROUPS: list[str] = [
    "Zarządzanie II gr1",
    "Zarządzanie II gr2",
    "Zarządzanie II gr3",
]
```

> **Tip:** run with `--dry-run` first (see below) to inspect what the parser
> finds before committing anything to the database.

### 3 — Test run

```bash
# Parse and print but don't change anything
python -m scraper.main --dry-run

# Force a full update (ignores stored hash)
python -m scraper.main --force

# Normal run (respects change detection)
python -m scraper.main
```

### 4 — Schedule (choose one)

#### Option A: system cron

```bash
chmod +x install_cron.sh
./install_cron.sh
```

This adds the following entry to your crontab:

```
0 6 * * * cd /path/to/SANet && python3 -m scraper.main >> logs/cron.log 2>&1
```

#### Option B: GitHub Actions

Push this repository to GitHub. The workflow in
`.github/workflows/daily_update.yml` will:

- Run automatically at **05:00 UTC** (= 06:00 Warsaw winter time).
- Commit the updated `zarzadzanie.html` and `data/` artefacts back to the repo.
- Upload the log as a run artefact (kept 14 days).

You can also trigger a manual run from the **Actions** tab with an optional
*force* flag.

---

## Tuning the parser

The PDF layout may differ from semester to semester. `parser.py` uses two
strategies:

| Strategy | When used |
|---|---|
| `pdfplumber` table extraction | When the page contains structured tables |
| Plain-text line scan | Fallback when no tables are detected |

If the parser returns 0 entries after a real PDF update:

1. Open the PDF and note the exact group label (e.g. `"Zarządzanie gr. 1"`).
2. Update `TARGET_GROUPS` in `config.py`.
3. Run `python -m scraper.main --force --dry-run` and check the log.
4. If columns are detected incorrectly, add explicit column-name hints to
   `parser._detect_columns()` or hard-code the column indices for that PDF.

---

## Change Detection Strategy

| Scenario | Behaviour |
|---|---|
| First run (no stored hash) | Treated as changed; full update runs |
| PDF binary unchanged | Exit 0 immediately; DB and HTML untouched |
| PDF updated | Old DB rows cleared atomically; fresh rows inserted; HTML regenerated |
| Download fails (network) | Logged, exit 1; stored hash/DB preserved |
| Parser returns 0 entries | Warning logged; DB **not** cleared (failsafe) |
| DB write fails | Rolled back; HTML **not** regenerated; exit 1 |

The failsafe rule (row 5 above) prevents accidentally wiping the live
schedule when the PDF format breaks — the old data stays visible until a
successful parse.

---

## Requirements

| Package | Purpose |
|---|---|
| `requests` | HTTP page scraping & PDF download |
| `beautifulsoup4` + `lxml` | Parsing HTML for the PDF `<a>` link |
| `pdfplumber` | PDF table & text extraction |

All other dependencies (`hashlib`, `sqlite3`, `json`, `logging`, `pathlib`)
are part of the Python standard library.
